\documentclass[a4paper,11pt]{article}
%\documentclass[a4paper,11pt,draft]{article}



\usepackage{graphicx}
%\usepackage[numbers,comma]{natbib}
\usepackage{natbib}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}

\input{defs}

\usepackage[T1]{fontenc}
\usepackage{times}
%
% BORDERS:
% HORIZONTAL:	210mm - 50.8mm = 159.2mm ROUNDED TO 16cm
% VERTICAL:	297mm - 50.8mm = 246.2mm ROUNDED TO 25cm
%
\setlength{\oddsidemargin}{0.0cm}
\setlength{\textwidth}{16.0cm}
\setlength{\topmargin}{-1.5cm}
\setlength{\textheight}{25.0cm}
\parindent = 0.0 truept
\parskip = 6.0 truept


\title{Nested Grid for \pion{}}
\author{Jonathan Mackey \textit{et al.}}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rationale}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Most of the simulations that are run with \pion{} are based around a point source of radiation and/or mass, usually a star.
For quite a lot of the simulations it is most useful to have very high resolution near the star, and the resolution requirements decrease with increasing distance.
This is not always true, for example in an expanding shockwave it sometimes becomes unstable only at large radius and one would want to resolve the thin shell.
But for many problems it is very important to resolve what is going on near the star.

Uniform grid simulations can get very expensive (especially in 3D) if high resolution is needed somewhere on the grid, because the cost of a simulation increases by a factor of $2^{D+1}$ when the resolution is increased by a factor of two (here $D$ is the number of spatial dimensions).
Consider then a \textit{nested grid}, where there are $N$ levels, and where each level has a computational grid that is the same as the coarser level but $2\times$ smaller in each dimension, and is centred on the origin.
Here each level has the same amount of work per timestep, but level $i$ must complete $2^i\times$ more timesteps than level 0.

For a concrete example, consider a $256^3$ uniform grid, integrated for a singe timestep, that requires $x$ core-hours of computation.
If we want $16\times$ more resolution, then a uniform grid calculation with $4096^3$ will require $16^4x = 65536x$ core-hours (CH).
With a nested grid, 4 levels of refinement will give the required resolution, and so the work required is
\begin{equation}
W(4) = x + 2^1x + 2^2x + 2^3x + 2^4x = 31x \;\; \mathrm{CH}\;.
\end{equation}
This is about $2100\times$ less computation than the uniform grid.
Similarly $W(1)$ requires $16/3\times$ less computation, $W(2)$ requires $256/7\times$ less, and $W(3)$ requires $4096/15\times$ less.
Resources are always finite, and so it makes sense to run on a nested grid for those cases where we only need high resolution near a certain point.

This was used for 2D simulations by \citet{FreHenYor03, FreHenYor06} of the circumstellar medium around a massive star that emits a strong wind.
I should look at how many levels they used, but it is really impressive how much computation you can save with this sort of refinement.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to have good (and straightforward) load-balancing, it would seem that the domain decomposition should be the same at each level, and each core should have one patch of the domain at each level.
This means that a $256^3$ base grid can easily be run with 512 cores (8 sub-domains along each direction) and the grid patches are still $32^3$ in size.
This is about the minimum patch size that still shows reasonable scaling.

So we should set up a new nested grid class that inherits from the parallel grid class.
The communication will be a bit more complicated because a coarse grid will need data from a few finer grid instead of being integrated.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Domain Decomposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A tree is a good structure for dealing with different levels of refinement on a grid, but might be overkill for a simple nested grid.
We will need to specify:
\begin{enumerate}
\item Number of spatial dimensions, $D$.
\item Aspect ratio of grid, $A_x:A_y:A_z$, e.g. 4:3:3, so that the tree has the right number of base nodes in each direction.
\item Number of processes (which is the number of domains on each level).
This should be a multiple of $A_xA_yA_z$ and a multiple of $A_xA_yA_z2^D$ to get cubic domains.
\item Size of coarsest domain, by specifying vectors $X_\mathrm{min}$ and $X_\mathrm{max}$.
\item Number of grid cells in each direction, $\{N_x,N_y,N_z\}$, on the coarsest grid level.
\item Centre of the nesting $\{x_n,y_n,z_n\}$.
\item Number of levels in the nest.
\end{enumerate}

We could also specify the resolution multiplier in going from a coarse level to the next refined level, but this is hard-coded to 2.
Similarly, we could specify the domain-size multiplier in going from one level to the next, but this is hard-coded to 0.5.

This requires setting up a data structure for each MPI process (all identical), and the process then finds its own location on each level, and finds which sub-domain on each level that it is responsible for.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Setting up a nested grid in \pion{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The simulation control class sets up a fixed grid, using the SimParams and MCMDcontrol data classes for the grid size and decomposition, the CellInterface class for calculating cell properties, and a finite volume solver class for calculating fluxes in and out of cells.
Some key parameters:
\begin{itemize}
\item Xmin, Xmax, Range, dx in SimPM and mpiPM.
\item dx, xmin in the cell interface.
\item The raytracer is attached to a grid, so I need one RT class for each grid.
\end{itemize}
The simplest way to deal with these is to set up new SimPM, mpiPM, CI classes for each level, although it would be better to split off the data that is level-independent into a global class, and make a new class for the level-dependent data.

I added a new struct called levels to SimPM, and it will contain the global grid dimensions for each level.

\bibliographystyle{apj_mod.bst}
\bibliography{refs}
\end{document}


