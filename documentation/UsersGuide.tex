%%%
%%% PION Users Guide
%%%
\documentclass[a4paper,11pt]{report}
%\documentclass[a4paper,11pt,draft]{article}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage{morefloats}
\usepackage{rotating}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{color,soul}
\usepackage[normalem]{ulem}
%
% THIS GIVES TIMES NEW ROMAN FONTS:
% http://www.terminally-incoherent.com/blog/2008/05/08/few-words-on-latex-fonts/
%
\usepackage[T1]{fontenc}
\usepackage{times}
%
% BORDERS:
% HORIZONTAL:	210mm - 50.8mm = 159.2mm ROUNDED TO 16cm
% VERTICAL:	297mm - 50.8mm = 246.2mm ROUNDED TO 25cm
%
\setlength{\oddsidemargin}{0.0cm}
\setlength{\textwidth}{16.0cm}
\setlength{\topmargin}{-1.5cm}
\setlength{\textheight}{25.0cm}
\parindent = 0.0 truept
\parskip = 6.0 truept

\input{./defs.tex}



\title{User's Guide for \pion{} version 1.0}
\author{Jonathan Mackey}
\begin{document}
\maketitle


\section*{Important Information}
This documentation applies to \pion{} version 1.0.

\pion{} is a grid-based code for astrophysical fluid dynamics.

\subsection*{Contact information and feedback}
The main developer of \pion{} is Jonathan Mackey.
You can contact him by email at

\hangindent=0.5cm
\quad \url{jmackey@cp.dias.ie}

or by post at:

\hangindent=0.5cm
\quad Dunsink Observatory\\
\quad Dunsink Lane\\
\quad Dublin 15\\
\quad D15 XR2R
\quad Ireland.

If you find a bug, please report it to J.~Mackey by email and file it as a bug at \href{https://bitbucket.org/jmackey/pion}{https://bitbucket.org/jmackey/pion}.
Ideally, please also submit a patch to fix the bug!

If you add new code segments and contribute them back to \pion{}, they are not subject to the same distribution restrictions as the rest of \pion{}, unless you want them to be, i.e.~you maintain control of the rights and restrictions to any code you write.


\subsection*{Terms of Use}
The terms of use are noted in detail in the file \texttt{pion/LICENCE.txt}.
Please read them, and if you don't agree with any of the conditions, then don't use \pion{}.

A noteworthy condition is that \pion{} comes with no warranty whatsoever.
Best efforts have been made to ensure the code accurately solves the equations it claims to solve, but there is no guarantee to this effect.
The code is developed for astrophysics, not for any practical purpose where failure or bugs might have material consequences (except for the developers' reputations and careers!).

\newpage
\addcontentsline{toc}{section}{Table of Contents}
\tableofcontents
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Quick Start Guide}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Running a simple test problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This guide shows you how to run the double Mach-reflection test, in serial and parallel, and to compare the results with expected behaviour.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview of Code Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Installation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Installation is reasonably straightforward, but there is one library which needs to be manually downloaded because it does not have a free licence.
The code is not really \emph{installed}, just compiled on a given computer.
The executables can then be moved to any directory you want and run on a particular problem.
It is quite rare that the code needs to be recompiled to run a different simulation -- a guiding design principle of the code is that most options should be set at run-time and not compile-time.
Here is how to compile the code:
\begin{enumerate}
\item
  If you have a copy of \pion{} as \texttt{pion1.tar.gz}, unzip/untar it into any directory you want.  The cd into this directory and the directory \verb|pion/| should have been created there.  cd into this directory.
\item
  The Sundials numerical integration library is used for some of the microphysics classes, so the file \verb|sundials-2.5.0.tar.gz| needs to be downloaded from \url{https://computation.llnl.gov/casc/sundials/download/download.html} and moved into the directory \verb|pion/extra_libraries/|.
\item
  Then from \verb|pion/| with a standard linux workstation you should be able to run the command \verb|./install.sh|.
  If this is successful then the \textsc{Silo}, \textsc{FITS}, and \textsc{Sundials} libraries should be installed to \verb|pion/extra_libraries/| and code executables should have been created in the current directory.
  The exectuables are \verb|pion_serial|, \verb|pion_parallel|, \verb|icgen_serial|, and \verb|icgen_parallel|.
\item
  If the executables are not there, then something went wrong, and you should run the commands in install.sh one-by-one to determine where things failed (probably in the library installation).
\end{enumerate}

The compile scripts will automatically detect if it is being compiled on the following systems:
\begin{itemize}
\item \emph{JUROPA} at J\"ulich Supercomputing Centre,
\item \emph{SuperMUC} at the Leibniz Rechenzentrum (LRZ) in Munich,
\item \emph{Phalanx} or \emph{Dougal} at UCL astrophysics dept.,
\item \emph{Dirac-II/Complexity} at the University of Leicester,
\item \emph{OS X 10.X}, with \emph{XCode} installed.
\end{itemize}
If none of these machines is identified then it is assumed that compilation is on a standard linux/UNIX workstation.
It has only been compiled on Debian-based linux distributions (Debian and Ubuntu), so I don't know if it works on Fedora/RedHat linux.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compilation on other machines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
If you want to compile it on a machine with e.g.\ Intel compilers, then you can either email me for help or you can try editing the compile scripts.
It is relatively straightforward to modify the compile scripts for other machines -- it is just a matter of locating the standard libraries, setting the MPI wrapper commands, and setting the \texttt{CC, CXX} environmental variables (Fortran is not needed, but one or other of the libraries may also need \texttt{FC} to be defined if not using gfortran).
As you can see from \verb|install.sh|, you need to edit the files
\verb|pion/extra_libraries/install_all_libs.sh|,
\verb|pion/bin_parallel/compile_code.sh|, and
\verb|pion/bin_serial/compile_code.sh|, and maybe also \verb|Makefile| in
\verb|pion/bin_parallel| and \verb|pion/bin_serial|.

The library install script \verb|pion/extra_libraries/install_all_libs.sh| detects machines by hostname, so you can add a loop for your host.
\texttt{NCORES} and \texttt{MAKE\_UNAME} are not so important, but \texttt{CC, CXX} and \texttt{FC} need to be set to your C, C++, and FORTRAN compilers.

The serial and parallel \verb|compile_code.sh| scripts have a similar structure, but here \texttt{MAKE\_UNAME} is important because the Makefiles have a section defining various quantities depending on the value of \texttt{MAKE\_UNAME}.
A new section needs to be created for the new machine in the Makefiles in \verb|pion/bin_serial| and \verb|pion/bin_parallel|.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to Code}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%
% Written in C++
% Object-oriented
% Logical separation of components is more important than execution speed.
% But if code is designed correctly it should be efficient!
% Should not be a conflict between OOP and efficient code.
%

Two executables are compiled -- an initial condition (IC) generator and the executable to run the code.
Both are based mostly on the same source files, but the IC generator has some files that only it uses, and the same for the simulation executable.
The IC generator reads in a parameter file and uses the information in this text file to put gas on the computational grid, setup boundaries, radiation and/or stellar wind sources and anything else that needs to be set.
It then writes all of this to a standard simulation output file, in either FITS\footnote{See \url{http://heasarc.gsfc.nasa.gov/fitsio/} for details.} or SILO\footnote{See \url{https://wci.llnl.gov/codes/silo/} for details} format.
These output files contain a header with all of the information from the parameter file (and some extra info besides) and then the binary data stored in architecture-independent format (i.e.\ the data can be read on any computer if you have a code routine to read it).
So the parameter file is only used by the IC generator.

The simulation code then reads in the IC file, starting with the header to get the parameters of the grid and the equations to be solved.
Then the command-line is parsed to see if any parameters need to be overriden, such as the time to finish the simulation, the type of Riemann solver to use, how often to output data, etc.
It then generates the grid, reads in the data from the IC file, and finally sets up the boundaries.
Following this, the equations solver is set up, the microphysics solver (if needed) and the raytracer (if needed).
Once everything has been set up the time integrator is called, and timestepping proceeds until something triggers the end of the simulation.
At this point the code quits.


Both SILO and FITS data can be read by the VisIt visualisation software package\footnote{\url{http://visit}}.
FITS can also be read by python, SAO-DS9 and IDL (although I haven't tested this).
I'm not sure if there are other visualisation tools for SILO data, but VisIt was designed around SILO so there is probably no reason to use a different visualisation package.
FITS data doesn't really output in parallel efficiently (each core writes its own file), so it is not recommended, but it is good for serial data.
For running on more than a few cores, SILO output is recommended, and this can be postprocessed and converted to FITS with one of the analysis programmes that come with \pion{} \hl{[CHECK IT WORKS!  Add a link]}.

For 1D and 2D serial calculations, it is often more useful to output text files which can be read by e.g.\ Gnuplot.
This is also an option in the parameter file and can be set as a command-line option.

To summarise, here is how to set up a simulation:
\begin{itemize}
\item Compile the code on the machine you want to run it on.
\item Edit a parameter file so that it will set up the simulation you want.
\item Run the IC generator with the parameter file as a command-line argument.
\item Check that the IC file is as expected, with e.g.\ VisIt.
\item Run the simulation code with the IC file as a command-line argument.
\item Wait for the simulation to finish.
\end{itemize}

Of course life is not simple, so there are a few things to note.

In the current setup, when running the code in parallel the same number of processes must be used for the IC generator, running the simulation code, and restarting from a checkpoint or output file \hl{[FIX THIS!]}.
This is because each process writes its own grid to part of an output file, and so expects to find its own grid when it reads an input file.
It would not be too difficult to change this so that the read function can access data written by any number of cores, but it is some way down the todo list.

The error checking is not guaranteed to be the same in the IC generator and simulation code, so it is possible that the IC generator will create a valid IC file but the simulation code will not be able to run it because some parameters are missing or can only be used in combination with certain values of other parameters.
I have tried to catch all of the possible conflicts at the IC generator stage, but still you should always check that a simulation is set up correctly with a small simulation (on a coarser grid) before you submit a big job on a supercomputer.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Parameter File}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The paramater file is an ASCII text file containing a list of paramters and their values.
Lines beginning with the \verb|#| key are ignored and can be used for comments.
Blank lines are also ignored.
Lines with parameters must begin with a single-word parameter name, followed by some white space, followed by the parameter value (no commas please! Tabs may be ok, I haven't checked).
There are a list of allowed parameters described in Table~\ref{tab:plist}.

\begin{sidewaystable}
  \centering
  \begin{tabular}{| p{2cm} | p{5cm} | p{5cm} | l | p{7cm} |}
    \hline
    Parameter Name & Description & Allowed Values & Default Value & Notes\\ 
    \hline
    ndim & Number of grid dimensions & 1,2,3 & n/a & Not all coordinate systems work with all dimensions, see below.
    \\
    coordinates & Type of coordinate system & spherical (1D), cylindrical (2D), cartesian (1-3D) & n/a & Only Cartesian works in 3D.
    \\
    eqn & System of Equations & euler, mhd, glm-mhd & n/a & mhd only useful in 1D, otherwise use glm-mhd which uses mixed-GLM divergence cleaning of \protect{\cite{DedKemKroEA02}}
    \\
    solver & Which Riemann solver & Hydro:1,2,3,4,5,6 MHD:1,4 & 1 & Hydro: 3=hybrid linear/exact solver is recommended.  MHD: 1=linear solver \citep{FalKomJoa98,RoeBal96}.
    Both: 4=Roe conserved vars solver \citep{StoGarTeuEA08} %Athena paper
    \\
    ics & Type of initial conditions to set up & See below for details & n/a & See below.
    \\
    ICfilename & Base name of initial conditions file & any unix filename & n/a & Don't include \verb|.silo| or \verb|.fits| at the end.
    \\
    OutputFile & Base name of output file & any unix filename & n/a &  Don't include \verb|.silo| or \verb|.fits| at the end.
    \\
    ntracer  & Number of tracer variables & $0-75$ & 0 & Must be integer.
    \\
    trtype   & String describing chemistry and tracers & e.g.\ \verb|NONE__colour| & n/a & example means no chemistry and one colour tracer.  The first six characters determine what kind of microphysics solver we are to use (see below for further details).
    \\
    InitIons & How to initialise tracer values & LEAVE, \textbf{other options} & n/a & LEAVE means the microphysics solver does not do anything to the values.
    \\
    Tracer0 & Initial value of tracer 0 & any double-precision number & ? & This can be overriden by some problem setups; see their descriptions for details.  This parameter may end up deprecated.
    \\
    Tracer1 & as above, and similar for all further tracers & as above & n/a & as above
    \\
    ... & ... & ... & ... & ... \\
    \hline
  \end{tabular}
  \caption{}
  \label{tab:plist}
\end{sidewaystable}


Possible values for initial conditions parameter \verb|ics| are given in the following list.  Note that many of the problems require extra specific parameters in the parameter file in order to set up a problem correctly.  These are described in the following subsections.
\begin{itemize}
  \item \verb|ShockTube| -- Sets up a 1D or 2D shock-tube problem.
  This requires additionally a number of parameters. [FILL IN].
  \item \verb|OrszagTang| -- Sets up the 2D \citet{OrsTan79} vortex test problem.[FILL IN].
  \item \verb|Uniform| -- Sets up uniform initial conditions.
  \item \verb|Advection| -- Sets up a simple problem advecting a feature across a periodic domain.
  \item \verb|AdvectSineWave| -- Can't remember! Advects a sine wave across the grid maybe.
  \item \verb|KelvinHelmholz| -- Sets up a Kelvin-Helmholz instability test problem.
  \item \verb|KelvinHelmholzStone| -- Sets up the version of the K-H test problem on Jim Stones code test page \url{http://www.astro.princeton.edu/~stone/FIXME}.
  \item \verb|FieldLoop| -- Sets up a 2D MHD Field Loop advection test.
  \item \verb|FieldLoopVz| -- Sets up a 2D Field loop advection test with a non-zero $\hat{z}$ component in the velocity.
  \item \verb|FieldLoopStatic| -- Static version of the Field Loop advection test (to see the diffusivity of the scheme without any advection).
  \item \verb|Jet| or \verb|JET| or \verb|jet| -- A 2D or 3D jet simulation, where the jet is injected along the symmetry axis in 2D or along the $\hat{x}$ axis in 3D.
  \item \verb|DoubleMachRef| -- The famous double Mach reflection test of \citet{ColWoo84} [CHECK REF!].
  \item \verb|RadiativeShock| -- Radiative shock test \citep[e.g.][]{InnGidFal87b} with wall boundary condition.
  \item \verb|RadiativeShockOutflow| -- Radiative shock with outflow downstream boundary condition instead of wall.
  \item \verb|LaserAblationAxi| -- Ablation of a target by a laser and the subsequent hydrodynamic evolution
  \item \verb|LaserAblation3D| -- Ablation of a target by a laser in 3D.
  \item \verb|ShockCloud| -- Planar shock from the $-\hat{x}$ direction hitting a stationary top-hat cloud.
  \item \verb|BlastWave| -- A blast wave test problem, in 1-3D and any of the allowed coordinate systems, with any requested microphysics.
  \item \verb|PhotoEvaporatingClump| -- Photoionisation problem with a radiation source and a dense clump embedded in a uniform background medium.
  \item \verb|PhotoEvaporatingClump2| -- Same as previous but with two clumps (e.g.\ for the problem in \citealt{LimMel03}).
  \item \verb|PhotoEvap_radial| -- Photoevaporation problem with a source in an ambient medium with a density profile.
  \item \verb|PhotoEvap_powerlaw| -- As above, with a power-law density profile.
  \item \verb|PhotoEvap_paralleltest| -- Not sure, something to do with photoionisation [FIX THIS!].
  \item \verb|PhotEvap_RandomClumps| or \verb|PhotEvap_RandomClumps2| or \verb|PERC| or \verb|PERC2| -- Second generation photoevaporation problem with random dense clumps placed in a uniform background and subject to ionising radiation.  Used by \citet{MacLim10} to set up calculations with randomly placed clumps.
  \item \verb|PhotEvap_MultiClumps_FixNum| or \verb|PhotEvap_MultiClumps_FixMass| or \verb|PE_MC_FN| or \verb|PE_MC_FM| -- Third generation photoevaporation problem used for the calculations involving multiple clumps placed in strategic positions in \citet{MacLim10,MacLim11b}.  The FixNum value uses a fixed number of clumps with random properties within a specified range, whereas the FixMass value uses a fixed mass of clump material and allocates it to clumps with random properties until all of the mass is used up.
  \item \verb|Clump_Spherical| or \verb|Clump_Axisymmetric| -- Don't know [LOOK THIS UP!].
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Data Input/Output}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
  \centering
  \caption{
    Summary of data input/output options for \pion{}.
  }
  \begin{tabular}{ l c l}
    %\hline
    Format & Binary? & Intended Usage \\
    \hline
    ASCII text & no   & serial, 1D and small 2D sims., debugging. \\
    FITS       & yes  & serial and small parallel sims. \\
    SILO       & yes  & serial, parallel, including HPC sims. \\
  \end{tabular}
  \label{tab:ioformats}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\textsc{Silo} I/O}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This is the ``native'' format used by \pion{} and should be used by default for parallel simulations.
For performance reasons it is strongly recommended when running on more than about 16 CPUs.
On supercomputers it is the only output format recommended.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\textsc{Fits} I/O}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textsc{Fits} is a flexible data format that, importantly, writes \emph{architecture-independent} files.
This means you don't need to worry about little-endian or big-endian, or fortran-unformatted data, or binary/ascii.
In addition, it is very well-supported by astronomical software, and there are GUI programs that can display FITS data.
It writes binary data, so it is fast and loss-less.

\pion{} can read and write \textsc{Fits} files, although the data I/O is not as simple for running in parallel as is the case for \textsc{Silo}.
What happens is that each MPI process writes its part of the grid to its own file, and each file also contains a part of the header.
To visualise the data afterwards you need to merge all of these files into a single \textsc{Fits} file, using the script \\
\verb|pion/analysis/merge_fits_files/stitchfits_compile.sh|,\\
which acts on a whole series of output files to automate the process.
So there is one extra post-processing step with \textsc{Fits} data when running on multiple CPUs before you can look at the results.

The simulation variables are writted in different HDUs (Header Data Units), depending on the physics included, and all values are in cgs units.
First the ordering for hydrodynamics (no magnetic field) \hl{CHECK THIS!}:
\begin{itemize}
\item HDU1 is always the simulation parameters; lots of parameter-value pairs.
\item HDU2 is gas density.
\item HDU3 is gas pressure.
\item HDU4 is the $x$ component of velocity
\item HDU5 is the $y$ component of velocity (even if 1D or 2D).
\item HDU6 is the $z$ component of velocity (even if 1D or 2D).
\item HDU7 and upwards are tracer variables (up to 5 of them, but probably only 2).
\item If doing radiative transfer, the next HDUs are column densities from the radiation source(s), one for each source.
\item The final HDU is gas temperature (or specific internal energy, if no microphysics are specified).
  For example, if not doing radiative transfer, and if we have two tracer variables, this is HDU9.
\end{itemize}

Now the ordering for MHD simulations (mostly similar, but with MHD variables in the middle) \hl{CHECK THIS!}:
\begin{itemize}
\item HDU1 is always the simulation parameters; lots of parameter-value pairs.
\item HDU2 is gas density.
\item HDU3 is gas pressure.
\item HDU4-6 are the $x$, $y$, $z$ components of velocity.
\item HDU7-9 are the $x$, $y$, $z$ components of the magnetic field. 
\item HDU10 is the GLM-Psi variable for divergence cleaning the magnetic field, if using GLM-MHD in 2D or 3D (which you should use!).
\item HDU11 and upwards are tracer variables (HDU10 and upwards if not using GLM-MHD).
\item If doing radiative transfer, the next HDUs are column densities from the radiation source(s), one for each source.
\item The third-last HDU is gas temperature (or specific internal energy, if no microphysics are specified).
  For example, if not doing radiative transfer, and for a 2D MHD simulation with GLM method for divergence cleaning, and if we have two tracer variables, this is HDU13.
\item The second last HDU is $\divb$, so we know what the errors are.
\item The final HDU is the total pressure (sum of the gas pressure, $n_\mr{tot}k_\mr{\textsc{b}}T$, and magnetic pressure, $B^2/8\pi$).
\end{itemize}

Each MPI process (on each CPU) reads and writes its own file, so if you restart a simulation it must use the same number of MPI processes as the previous run.
Also, the same number of MPI processes must be used for writing the initial conditions file as for running the simulation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ASCII Text I/O}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Text I/O only works for serial code, and is only designed for 1D and 2D simulations.
It writes a text file for each simulation output, and each line of the file contains the variables (including position) for a cell.
For 2D simulations, columns of constant $y$ are written in order of increasing $x$ position, and a blank line is inserted at the end of each column.
This is so that it can be plotted simply with e.g.\ Gnuplot's 'splot' function.
The first lines of each text file contain the simulation time (in seconds), the timestep, and the list of variables that are written.
The ordering of variables is basically the same as for \textsc{Fits} files, except that the cell-centred positions of each cell are written in the first column for 1D or first two columns for 2D.




\bibliographystyle{./apj.bst}
\bibliography{./refs.bib}

\end{document}
